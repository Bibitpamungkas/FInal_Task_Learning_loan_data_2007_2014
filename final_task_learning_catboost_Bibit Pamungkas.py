# -*- coding: utf-8 -*-
"""Final Task Learning CatBoost.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hOiYOeQaMCZMMptWHi0RlHeBMwPNqoYW

# **Final Task Prediksi Data Peminjaman**

## **Pembacaan dan Review Data`**

#### **Melihat Data**

##### **Membaca dan mengambil file yang ada di dalam google drive yang dipakai**

Melakukan penyambungan penyimpanan google drive dan google collab untuk pengambilan data
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd /content/gdrive/My\ Drive/Colab\ Notebooks/rakamin

"""Menampilkan isi yang ada di dalam penyimpanan google drive yang dipakai"""

ls

"""##### **Menampilkan 5 Data Teratas**"""

import pandas as pd
dt = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/rakamin/loan_data_2007_2014.csv')
dt.head()

"""#### **Mendapatkan Informasi dan Membuat Kolom Baru**

##### **Melihat Informasi Data Secara Keseluruhan**

Menampilkan statistik deskriptif
"""

dt.describe()

"""Menampilkan informasi objek"""

dt.info()

"""Melakukan deteksi validasi nilai pada objek"""

dt.notnull()

"""Melakukan pengecekan jumlah nilai yang hilang pada objek"""

nulls = dt.isnull().sum().to_frame()
for index, row in nulls.iterrows():
  print(index, row[0])

"""Menampilkan tipe data dari setiap kolom"""

typess = dt.dtypes.to_frame()
for index, row in typess.iterrows():
  print(index, row[0])

"""##### **Menambah Kolom Baru Sebagai Kolom Target Prediksi**

Mengecek kolom target
"""

dt.loan_status.value_counts()

"""Membuat kolom baru *good_bad_loan*"""

import numpy as np

good_loans = ['Current', 'Fully Paid', 'In Grace Period',
              'Does not meet the credit policy. Status:Fully Paid']

dt['good_bad_loan'] = np.where(dt['loan_status'].isin(good_loans), 1, 0)

"""## **Data Preprocessing`**

#### **Melakukan Feature Selection**

##### Melakukan Penghilangan Kolom Berdasarkan Identifikasi yang Dilakukan

Melakukan penghilangan kolom berdasarkan penilaian pribadi terkait kegunaan kolom tersebut pada model
"""

dt.drop(columns=['Unnamed: 0','id','member_id','url','title','desc','zip_code','emp_title','issue_d', 'loan_status',
                 'pymnt_plan', 'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',
                'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',
                'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d','last_credit_pull_d','sub_grade','funded_amnt','funded_amnt_inv'], inplace=True)

"""##### Melakukan Penilaian kembali berdasarkan data

Pengecekan kolom yang memiliki data kosong diatas 50%
"""

cek_kolom = dt.isna().mean()
cek_kolom[cek_kolom > 0.5]

"""Pengecekan kolom yang memiliki nilai unik kurang dari 2 atau nilai uniknya sama dengan panjang data frame"""

dt.nunique()[(dt.nunique()<2) | (dt.nunique() == len(dt))]

"""##### Melakukan penghilangan kolom berdasarkan penilaian data kosong diatas 50% dan nilai unik kurang dari 2"""

hapus_kolom1 = list(dt.nunique()[(dt.nunique()<2) | (dt.nunique() == len(dt))].index)
hapus_kolom2 = list(cek_kolom[cek_kolom > 0.5].index)
kolom12= set(hapus_kolom1+hapus_kolom2)

dt.drop(kolom12, axis=1, inplace=True)
dt.shape

"""Menampilkan Heatmap"""

import matplotlib.pyplot as plt
import seaborn as sns

#Check correlation
plt.figure(figsize=(24,24))
sns.heatmap(dt.corr(), annot=True, annot_kws={'size':14})

"""#### **Melakukan Handling Missing Data**

##### Melakukan Pengecekan Missing Value

Melakukan perhitungan total dari data yang kosong
"""

dt.isnull().sum()

"""Mengambil kolom yang memiliki nilai null"""

dt_columns=dt.columns[dt.isnull().any()].tolist()
dt[dt_columns].isnull().sum()*100/len(dt)

"""##### Melakukan Drop Kolom yang Memiliki Banyak Missing Value

Terdapat 3 kolom yang terdapat missing value sebanyak 15% dari data
"""

dt.dropna(subset = ['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim','revol_util'], inplace = True)

"""Reset Index"""

dt.reset_index(drop= True, inplace = True)

"""#### **Melakukan Transformasi Tipe Data Variabel Kontinu**

Terdapat 4 variabel yang akan diubah
"""

kontinu_cols = ['term', 'emp_length', 'earliest_cr_line']
dt[kontinu_cols]

"""##### Transformasi varabel term"""

dt['term']

"""Konversi tipe data menjadi numerik"""

dt['term'] = pd.to_numeric(dt['term'].str.replace(' months', ''))
dt['term']

"""##### Transformasi variabel emp_length

Menampilkan nilai unik
"""

dt['emp_length'].unique()

"""Konversi tipe data menjadi numerik"""

emp_map = {
    '< 1 year' : '0',
    '1 year' : '1',
    '2 years' : '2',
    '3 years' : '3',
    '4 years' : '4',
    '5 years' : '5',
    '6 years' : '6',
    '7 years' : '7',
    '8 years' : '8',
    '9 years' : '9',
    '10+ years' : '10'
}

dt['emp_length'] = dt['emp_length'].map(emp_map).fillna('0').astype(int)
dt['emp_length'].unique()

"""##### Transformasi variabel earliest_cr_line"""

dt['earliest_cr_line']

"""Transformasi Menjadi Objek"""

dt['earliest_cr_line'] = pd.to_datetime(dt['earliest_cr_line'], format = '%b-%y')
dt['earliest_cr_line'] = (dt['earliest_cr_line']  - dt['earliest_cr_line'] .min())  / np.timedelta64(1,'D')
dt['earliest_cr_line']

dt.isnull().sum()

"""#### **Melakukan Pengecekan Fitur Tipe Data**"""

dt.info()

"""## **Exploratory Data Analysis`**"""

def annot_plot(ax,w,h):
  ax.spines['top'].set_visible(False)
  ax.spines['right'].set_visible(False)
  for p in ax.patches:
      ax.annotate('{0:.1f}'.format(p.get_height()), (p.get_x()+w, p.get_height()+h))

import matplotlib.pyplot as plt
import seaborn as sns

ax = sns.countplot(dt ,x='good_bad_loan')
annot_plot(ax, 0.5,1)

plt.title("Bar Chart")
plt.xlabel('Bad and Good Loan')
plt.ylabel('Total')

plt.show()

pd.crosstab(dt.grade,dt['good_bad_loan']).plot(kind="bar",figsize=(15,6),color=['#1CA53B','#AA1111'])
plt.title('Bad and Good Loan Based Grade')
plt.xlabel('Sex (0 = Bad, 1 = Good)')
plt.xticks(rotation=0)
plt.ylabel('Total')
plt.show()

numeric_features = list(dt.select_dtypes(["float64" , "int64",'int32']).columns)
n = numeric_features.index('good_bad_loan')
del n

cat_cols = list(dt.select_dtypes(["object","datetime64[ns]"]).columns)

target = "good_bad_loan"

print(f'numeric_features:\n{numeric_features}\n\ncategorical_features:\n{cat_cols}\n\ntarget:\n{target}')

"""## **Encoding & Normalization`**

### **One Hot Encoding**
"""

from sklearn.preprocessing import OneHotEncoder
cat_cols = [col for col in dt.select_dtypes(include='object').columns.tolist()]
onehot_cols = pd.get_dummies(dt[cat_cols], drop_first=True)
onehot_cols

"""### **Normalization**"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

num_cols = [col for col in dt.columns.tolist() if col not in cat_cols + ['good_bad_loan']]
normal = pd.DataFrame(scaler.fit_transform(dt[num_cols]), columns=num_cols)

normal

"""Final Data"""

final_data = pd.concat([onehot_cols, normal, dt[['good_bad_loan']]], axis=1)
final_data.head()

"""## **Modeling**

### **Spliting Data**
"""

!pip install catboost
from catboost import CatBoostClassifier, Pool, metrics, cv
from sklearn.model_selection import train_test_split
import sklearn.metrics as metrics
import numpy as np
from sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score,f1_score,classification_report

x = final_data.drop(['good_bad_loan'], axis=1)
y = final_data['good_bad_loan']

import numpy as np
cat_features = np.where(x.dtypes != float)[0]
print(cat_features)

X_train, X_test, y_train, y_test = train_test_split(x, y ,test_size=0.2, random_state=14)
y_train.value_counts()

"""### **SMOTE**"""

from imblearn.over_sampling import SMOTE
from collections import Counter

smote = SMOTE(random_state=14)
# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(X_train, y_train)


print('Dataset Sebelum di SMOTE', Counter(y_train))
print('Dataset setelah di SMOTE', Counter(y_smote))

"""### **Model Training Data**"""

model = CatBoostClassifier(iterations= 500,
    random_seed=42,
    logging_level='Silent')
model.fit(x_smote, y_smote, eval_set=(X_test,y_test),
          use_best_model=True, early_stopping_rounds=10,
          cat_features=cat_features,verbose=1000)

"""## **Classification Report**"""

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

#accuracy
accuracy = accuracy_score(y_test, predictions)
print("Accuracy: %.2f%%" % (accuracy * 100.0))

#precision
precision = precision_score(y_test, predictions)
print("Precision : %.2f%%" % (precision * 100.0))

#recall
recall = recall_score(y_test, y_pred)
print("Recall : ",recall )

#F1 SCORE
f1score = f1_score(y_test.T,y_pred, average='macro')
print("F1 Score : ",f1score )

c_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])
fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))

sns.heatmap(pd.DataFrame(c_matrix), annot=True, cmap="Blues" ,fmt='g',
            xticklabels=['Bad Loan', 'Good Loan'],
            yticklabels=['Bad Loan', 'Good Loan'],)
ax1.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion Matrix', y=1.1,fontsize=14)
plt.show()

from sklearn.metrics import classification_report
ypred = model.predict(X_test)
print(classification_report(y_test, ypred))

"""## **Feature Importance**"""

importances = model.get_feature_importance(type='PredictionValuesChange')
feature_importancess = pd.Series(importances, index=x.columns).sort_values()
feature_importancess

important_features = feature_importancess[feature_importancess > 0.1]
print(important_features)

feature_importances =important_features
sorted_idx = np.argsort(feature_importances)
fig = plt.figure(figsize=(12, 6))
plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), np.array(X_test.columns)[sorted_idx])
plt.title('Feature Importance')

"""## **HyperParameter Tunning**"""

from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as stats

# defining parameter range
param_grid = {
    'max_depth': stats.randint(9, 10),
    'learning_rate': stats.uniform(0.01, 0.1),
    'subsample': stats.uniform(0.05, 0.5)
}

grid = RandomizedSearchCV(model, param_grid, cv=7, scoring='accuracy')

# fitting the model for grid search
grid.fit(x_smote, y_smote)

# print best parameter after tuning
print(grid.best_params_)
print("Best score: ", grid.best_score_)

# print how our model looks after hyper-parameter tuning
print(grid.best_estimator_)